# Microservicio TinyLlama con FastAPI

Bienvenido al **Microservicio TinyLlama**, un servicio RESTful basado en **FastAPI** que expone un modelo de lenguaje natural optimizado llamado **TinyLlama**. Este modelo estÃ¡ diseÃ±ado para ofrecer consultas SQL dinÃ¡micas utilizando lenguaje natural, con un bajo consumo de recursos de hardware gracias a su integraciÃ³n con **Docker**.

Este microservicio permite a partir de entradas en lenguaje natural y puede interactuar, para recuperar informaciÃ³n.

# CaracterÃ­sticas

- **OptimizaciÃ³n de Recursos**: Utiliza Docker para ejecutar el modelo **TinyLlama** de manera eficiente en tÃ©rminos de recursos de hardware.
- **Microservicio con FastAPI**: ExposiciÃ³n de la funcionalidad como un microservicio RESTful, fÃ¡cil de integrar con otros sistemas.
  
## Arquitectura

![chatbot](img/Tinyllama.png)

### Flujo General

- **1. Entrada del Usuario**: Se reciben consultas en lenguaje natural a travÃ©s de la API REST.
- **2. GeneraciÃ³n de Consultas**: El modelo **TinyLlama** genera la respuesta correspondiente a partir de la consulta en lenguaje natural.
- **3. Respuesta al Usuario**: Los resultados de la consulta se devuelven en formato amigable para el usuario.

# InstalaciÃ³n

### Clonar el proyecto

```
https://github.com/lriveraBanco/chat-with-lz.git
```

### ConfiguraciÃ³n el proyecto

Seleccionar el proyecto : Moverse al directorio principal

```
cd microservices_ia
```

### Crear entorno virtual si tienes recursos CPU o GPU para ejecutar este proyecto

```
python3 -m venv venv
```

### Activar entorno virtual

**Para Linux/MacOS**

```
source venv/bin/activate
```

**En Windows:**

```
venv\Scripts\activate
```

### Instalar dependencias

```
pip install -r requirements.txt
```

### ejecutar microservicio de manera local

```
uvicorn main:app --host 0.0.0.0 --port 80 --reload
```

## Dockerizado
### Configurar Docker (si usas Docker para TinyLlama)

Si prefieres ejecutar el modelo en Docker, sigue estos pasos:

1. **Construir la Imagen Docker**:

```
docker build -t nombre_imagen .
```

2. **Ejecutar el Contenedor Docker**:

```
docker run --name nombre_contenedor -d -p 8080:80 nombre_imagen
```

### ConfiguraciÃ³n de la API del Microservicio FastAPI y USO

AsegÃºrate de que el microservicio FastAPI estÃ© corriendo antes de consumir el API:
![chatbot](img/postman.png)

```
http://127.0.0.1:8000/ai/chatbot/tinyllama
```



### 2. Interactuar con el Chatbot

Escribe consultas en lenguaje natural, como:

- **"Que es LLMs"**
- **"Â¿que es tinyllama?"**
- **"Capital de Colombia"**

El chatbot generarÃ¡ respuestas, a tus multiples preguntas.


**Estructura bÃ¡sica del proyecto:**

```plaintext
tinyllama-microservicio/
â”‚
â”œâ”€â”€ api_v1/
â”‚   â”œâ”€â”€ logic
|       â”œâ”€â”€tinyllama.py
â”‚   â”œâ”€â”€ routes
|       â”œâ”€â”€tinyllama.py
|   â”œâ”€â”€api.py
|   â”œâ”€â”€events.py
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ Tinyllama.png            
â”œâ”€â”€ Dockerfile         # Para la construcciÃ³n del contenedor Docker
â”œâ”€â”€ docs.py
â”œâ”€â”€ requirements.txt   # Dependencias del proyecto
â”œâ”€â”€ main.py            
â””â”€â”€ README.md          # Este archivo
```

### Bibliotecas y Herramientas Clave

**FastAPI:** Framework para crear la API RESTful.
**Uvicorn:** Servidor ASGI para ejecutar FastAPI.
**Docker:** Contenedor para ejecutar el modelo TinyLlama de manera eficiente.
**TinyLlama:** El modelo de lenguaje natural que genera respuesta a tus consultas.


## Contribuciones

**Si deseas contribuir a este proyecto, sigue estos pasos:**

1. Haz un fork del repositorio.
2. Crea una nueva rama (`git checkout -b feature-nueva-funcionalidad`).
3. Realiza tus cambios y haz commit (`git commit -m 'Agrega nueva funcionalidad'`).
4. Sube los cambios a la rama (`git push origin feature-nueva-funcionalidad`).
5. Abre un Pull Request.

## Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT. Consulta el archivo LICENSE para mÃ¡s detalles.

## Contacto

Leandro Rivera: <lrivera@bancolombia.com.co>

### Â¡Feliz CodificaciÃ³n! ğŸš€

Si encuentras Ãºtil este proyecto, Â¡dale una â­ en GitHub! ğŸ˜Š
