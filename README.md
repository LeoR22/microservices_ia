# Microservicio TinyLlama con FastAPI

Bienvenido al **Microservicio TinyLlama**, un servicio RESTful basado en **FastAPI** que expone un modelo de lenguaje natural optimizado llamado **TinyLlama**. Este modelo est√° dise√±ado para ofrecer consultas SQL din√°micas utilizando lenguaje natural, con un bajo consumo de recursos de hardware gracias a su integraci√≥n con **Docker**.

Este microservicio genera consultas SQL a partir de entradas en lenguaje natural y puede interactuar con bases de datos, como Impala, para recuperar informaci√≥n.

# Caracter√≠sticas

- **Consultas SQL en Lenguaje Natural**: Convierte consultas en lenguaje natural a SQL ejecutable.
- **Optimizaci√≥n de Recursos**: Utiliza Docker para ejecutar el modelo **TinyLlama** de manera eficiente en t√©rminos de recursos de hardware.
- **Integraci√≥n con Bases de Datos**: Conecta con **Impala** para ejecutar las consultas generadas.
- **Microservicio con FastAPI**: Exposici√≥n de la funcionalidad como un microservicio RESTful, f√°cil de integrar con otros sistemas.
  
## Arquitectura

![chat-lz](docs/chat_lz.png)

### Flujo General

**1. Entrada del Usuario**: Se reciben consultas en lenguaje natural a trav√©s de la API REST.
**2. Generaci√≥n de Consultas SQL**: El modelo **TinyLlama** genera la consulta SQL correspondiente a partir de la consulta en lenguaje natural.
**3. Ejecuci√≥n de la Consulta**: La consulta SQL es ejecutada en la base de datos **Impala**.
**4. Respuesta al Usuario**: Los resultados de la consulta se devuelven en formato amigable para el usuario, junto con la consulta SQL generada.

# Instalaci√≥n

### Clonar el proyecto

```
https://github.com/lriveraBanco/chat-with-lz.git
```

### Configuraci√≥n el proyecto

Seleccionar el proyecto : Moverse al directorio principal

```
cd chat-with-lz
```

### Crear entorno virtual

```
python3 -m venv venv
```

### Activar entorno virtual

**Para Linux/MacOS**

```
source venv/bin/activate
```

**En Windows:**

```
venv\Scripts\activate
```

### Instalar dependencias

```
pip install -r requirements.txt
```

### Configurar Docker (si usas Docker para TinyLlama)

Si prefieres ejecutar el modelo en Docker, sigue estos pasos:

1. **Construir la Imagen Docker**:

```
pip install -r requirements.txt
```

2. **Ejecutar el Contenedor Docker**:

```
pip install -r requirements.txt
```

3. **Verifica que el contenedor est√° funcionando correctamente ejecutando**:

```
pip install -r requirements.txt
```

### Ejecutar el Microservicio FastAPI

Para iniciar el microservicio, ejecuta:

```
pip install -r requirements.txt
```

### Configuraci√≥n de la API del Microservicio FastAPI

Aseg√∫rate de que el microservicio FastAPI est√© corriendo antes de iniciar la aplicaci√≥n Streamlit. Puedes configurarlo en tu archivo .env o directamente en el c√≥digo de Streamlit con la URL del microservicio:

```
FASTAPI_URL=http://127.0.0.1:8000/query
```

# USO

### 1. Ejecutar la Aplicaci√≥n de Streamlit

Inicia la aplicaci√≥n con el siguiente comando:

```
streamlit run app.py
```

### 2. Interactuar con el Chatbot

Escribe consultas en lenguaje natural, como:

- **"Muestra los registros m√°s recientes para la API X."**
- **"¬øCu√°l es el esquema de la tabla Y?"**
- **"Obt√©n el promedio de usuarios por API en los √∫ltimos tres meses."**

El chatbot generar√° consultas SQL, las ejecutar√° y mostrar√° los resultados.
Use the sidebar to initialize the connection to the proceso_apis database.

**Estructura b√°sica del proyecto:**

```plaintext
tinyllama-microservicio/
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ microservice_architecture.png
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py        # Archivo principal con FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ model.py       # L√≥gica del modelo TinyLlama
‚îÇ   ‚îú‚îÄ‚îÄ utils.py       # Funciones auxiliares
‚îú‚îÄ‚îÄ Dockerfile         # Para la construcci√≥n del contenedor Docker
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias del proyecto
‚îú‚îÄ‚îÄ .env               # Variables de entorno
‚îî‚îÄ‚îÄ README.md          # Este archivo
```

### Bibliotecas y Herramientas Clave

**FastAPI:** Framework para crear la API RESTful.
**Uvicorn:** Servidor ASGI para ejecutar FastAPI.
**Docker:** Contenedor para ejecutar el modelo TinyLlama de manera eficiente.
**TinyLlama:** El modelo de lenguaje natural que genera consultas SQL.
**Impala:** Base de datos para ejecutar las consultas generadas.

## Contribuciones

**Si deseas contribuir a este proyecto, sigue estos pasos:**

1. Haz un fork del repositorio.
2. Crea una nueva rama (`git checkout -b feature-nueva-funcionalidad`).
3. Realiza tus cambios y haz commit (`git commit -m 'Agrega nueva funcionalidad'`).
4. Sube los cambios a la rama (`git push origin feature-nueva-funcionalidad`).
5. Abre un Pull Request.

## Licencia

Este proyecto est√° licenciado bajo la Licencia MIT. Consulta el archivo LICENSE para m√°s detalles.

## Contacto

Leandro Rivera: <lrivera@bancolombia.com.co>

### ¬°Feliz Codificaci√≥n! üöÄ

Si encuentras √∫til este proyecto, ¬°dale una ‚≠ê en GitHub! üòä
